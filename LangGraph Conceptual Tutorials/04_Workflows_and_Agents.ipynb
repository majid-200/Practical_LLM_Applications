{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3784a9be-8d30-4e14-8bd0-b5d986c6d055",
   "metadata": {},
   "source": [
    "# LANGGRAPH DESIGN PATTERNS - Complete Reference Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf31d9-84ce-4d3a-9839-4593a279a87a",
   "metadata": {},
   "source": [
    "    This file demonstrates 6 core LangGraph patterns for building AI workflows:\n",
    "    \n",
    "    1. PROMPT CHAINING     - Sequential LLM calls with quality gates\n",
    "    2. PARALLELIZATION     - Run multiple LLMs simultaneously\n",
    "    3. ROUTING             - Dynamic routing based on input\n",
    "    4. ORCHESTRATOR-WORKER - Divide work among parallel workers\n",
    "    5. EVALUATOR-OPTIMIZER - Self-improving loops with feedback\n",
    "    6. AGENTS              - LLMs that use tools\n",
    "    \n",
    "    PATTERN COMPARISON:\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Pattern            â”‚ When to Use      â”‚ Example Use Case            â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚ Prompt Chaining    â”‚ Step-by-step     â”‚ Draft â†’ Edit â†’ Polish       â”‚\n",
    "    â”‚ Parallelization    â”‚ Independent work â”‚ Generate story + joke + poemâ”‚\n",
    "    â”‚ Routing            â”‚ Conditional flow â”‚ Route to story/joke/poem    â”‚\n",
    "    â”‚ Orchestrator       â”‚ Split work       â”‚ Write report sections       â”‚\n",
    "    â”‚ Evaluator          â”‚ Quality control  â”‚ Generate â†’ Grade â†’ Retry    â”‚\n",
    "    â”‚ Agents             â”‚ Use tools        â”‚ Calculator with arithmetic  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fec11f-bcff-4082-bf9e-bc233e4098ab",
   "metadata": {},
   "source": [
    "## SETUP: Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22bb240-0eac-478f-bb80-08eb6ca7080d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\miniconda3\\envs\\Ollama\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"qwen3:8b\",\n",
    "    model_provider=\"ollama\",\n",
    "    temperature=0  # 0 = deterministic (same input â†’ same output)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bfcff-fe2f-4671-abaa-9a7573a40884",
   "metadata": {},
   "source": [
    "## FOUNDATION: LLM Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4fb040-6be8-4668-b60b-54f0dc17111c",
   "metadata": {},
   "source": [
    "    Before diving into patterns, understand how to AUGMENT LLMs:\n",
    "    \n",
    "    1. STRUCTURED OUTPUT - Force LLM to return specific format\n",
    "    2. TOOL BINDING - Give LLM access to functions\n",
    "    \n",
    "    These augmentations are the building blocks for all patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741d6a8-cd93-4f30-bcb7-c7c858321aa2",
   "metadata": {},
   "source": [
    "### Augmentation 1: Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff074dac-3eed-4cc5-9e83-fe888a6eaa76",
   "metadata": {},
   "source": [
    "    Schema for structured output.\n",
    "    \n",
    "    Pydantic models define EXACTLY what fields the LLM should return.\n",
    "    \n",
    "    Without this: LLM might return free-form text\n",
    "    With this: LLM MUST return dict matching this schema\n",
    "    \n",
    "    Visual Comparison:\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Regular LLM:                                                            â”‚\n",
    "    â”‚ Input: \"How does X relate to Y?\"                                        â”‚\n",
    "    â”‚ Output: \"Here's a search query: 'X and Y relationship'. This is         â”‚\n",
    "    â”‚          relevant because...\"  â† Unstructured text                      â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Structured LLM:                                                         â”‚\n",
    "    â”‚ Input: \"How does X relate to Y?\"                                        â”‚\n",
    "    â”‚ Output: {                                                               â”‚\n",
    "    â”‚   \"search_query\": \"X and Y relationship\",                               â”‚\n",
    "    â”‚   \"justification\": \"This query directly addresses...\"                   â”‚\n",
    "    â”‚ } â† Guaranteed dict format                                              â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce30ca8-0e6e-4ab4-8aec-43936410bebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Example: Structured Output]\n",
      "Search Query: How does Calcium CT score relate to high cholesterol?\n",
      "Justification: The user is asking about the relationship between Calcium CT score and high cholesterol. This is a medical question that requires an explanation of how these two factors are connected in terms of cardiovascular health. The answer should cover the role of calcium in arteries, the impact of high cholesterol on plaque formation, and how the Calcium CT score is used to assess cardiovascular risk. It should also mention the importance of lifestyle changes and medical management for both conditions. The answer should be clear, concise, and provide actionable information for the user to understand the connection and take appropriate steps for their health. The user might be concerned about their own health or that of a family member, so the response should be informative and reassuring, emphasizing the importance of early detection and management. The answer should also highlight the role of healthcare professionals in interpreting the Calcium CT score and managing cholesterol levels. The user might be looking for a comprehensive understanding of the relationship between these two factors, so the answer should cover both the physiological mechanisms and the clinical implications. The answer should also address potential misconceptions, such as the belief that a high Calcium CT score is always a sign of severe disease, and explain that it is a risk factor but not a definitive diagnosis. The user might also be interested in how lifestyle changes can affect both conditions, so the answer should include recommendations for diet, exercise, and other preventive measures. The answer should be structured in a logical flow, starting with an explanation of each concept, then discussing their relationship, and finally providing practical advice for managing both conditions. The language should be accessible, avoiding excessive medical jargon while still being accurate. The answer should also mention the importance of regular check-ups and monitoring for individuals with high cholesterol or a high Calcium CT score. The user might be seeking reassurance that they can take control of their health through lifestyle modifications and medical interventions, so the answer should emphasize the positive aspects of early detection and proactive management. The answer should also touch on the role of medications in managing high cholesterol and how they can complement lifestyle changes. The user might be interested in the latest research or guidelines on this topic, so the answer should reference current medical standards and recommendations. The answer should be thorough but not overwhelming, providing enough information to inform the user without causing unnecessary alarm. The user might also be looking for a comparison between Calcium CT score and other cardiovascular risk factors, so the answer should briefly mention how this score fits into the broader context of heart disease risk assessment. The answer should conclude with a summary of the key points and a call to action for consulting a healthcare professional for personalized advice. The user might have additional questions or concerns that were not explicitly mentioned, so the answer should be open-ended and encourage further inquiry. The answer should also consider the possibility that the user might be in a different country or have access to different healthcare systems, so the recommendations should be general and applicable to a wide range of situations. The user might be interested in the cost-effectiveness of certain interventions, so the answer should mention that while lifestyle changes are cost-effective, medical treatments may be necessary depending on individual risk factors. The answer should also address the importance of a holistic approach to health, considering both cholesterol management and cardiovascular risk assessment through tools like the Calcium CT score. The user might be looking for a balance between medical advice and personal responsibility, so the answer should encourage proactive health management while acknowledging the role of professional medical care. The answer should be structured to allow for easy reading and comprehension, using clear headings and bullet points if necessary, but since the user requested a natural, conversational tone, the answer should be written in a flowing, easy-to-read manner. The answer should also be free of errors and based on accurate, up-to-date medical information. The user might be concerned about the implications of a high Calcium CT score, so the answer should provide reassurance that while it is a significant risk factor, it is not an automatic diagnosis and that there are effective ways to manage both high cholesterol and the associated risks. The answer should also mention the importance of a healthy lifestyle in reducing both high cholesterol and the risk of arterial calcification, reinforcing the idea that these are interconnected conditions that can be managed through a combination of medical and lifestyle interventions. The user might be interested in the long-term benefits of managing both conditions, so the answer should highlight the potential for improved cardiovascular health and reduced risk of heart disease. The answer should also touch on the role of diet, exercise, and stress management in both cholesterol control and reducing arterial calcification. The user might be looking for practical steps they can take immediately, so the answer should include specific recommendations for diet, exercise, and other lifestyle changes. The answer should also mention the importance of regular monitoring and follow-up with healthcare providers to ensure that both cholesterol levels and the Calcium CT score are being effectively managed. The user might be concerned about the accuracy of the Calcium CT score, so the answer should explain how this test is performed and its reliability in assessing cardiovascular risk. The answer should also address potential limitations of the Calcium CT score, such as its focus on calcium deposits rather than other aspects of plaque composition, and how this might influence the interpretation of results. The user might be interested in how the Calcium CT score compares to other risk assessment tools, such as the Framingham Risk Score or the Reynolds Risk Score, so the answer should briefly mention these comparisons. The answer should also consider the role of genetics and family history in both high cholesterol and the development of arterial calcification, emphasizing the importance of a personalized approach to risk management. The user might be looking for a comprehensive overview of the topic, so the answer should cover all these aspects while maintaining clarity and conciseness. The answer should be written in a way that is easy to understand, even for someone without a medical background, and should avoid overly technical terms unless necessary. The user might also be interested in the psychological impact of receiving a high Calcium CT score or a high cholesterol diagnosis, so the answer should address the importance of mental health and support in managing these conditions. The answer should conclude by reinforcing the message that while the Calcium CT score and high cholesterol are related, they are manageable conditions with the right approach, and that seeking professional medical advice is the best course of action for individualized care.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        None, description=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "# Augment the LLM with schema\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "# Invoke - guaranteed to return SearchQuery format\n",
    "print(\"\\n[Example: Structured Output]\")\n",
    "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "print(f\"Search Query: {output.search_query}\")\n",
    "print(f\"Justification: {output.justification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47311b45-5d10-47e7-8ab5-9cbb0537bd0e",
   "metadata": {},
   "source": [
    "### Augmentation 2: Tool Binding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757300d-a925-4841-8979-32b0b360f704",
   "metadata": {},
   "source": [
    "    Tool Binding Flow:\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ User: \"What is 2 times 3?\"                                           â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  llm_with_tools      â”‚\n",
    "              â”‚  (knows about tools) â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚ Option A: Call tool    â”‚ Option B: Answer directly\n",
    "            â”‚ tool_calls=[           â”‚ content=\"6\"\n",
    "            â”‚   {name: \"multiply\",   â”‚\n",
    "            â”‚    args: {a:2, b:3}}   â”‚\n",
    "            â”‚ ]                      â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a314be-7d27-42af-98e0-e79310473e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Example: Tool Binding]\n",
      "LLM decided to use tool: multiply\n",
      "With arguments: {'a': 2, 'b': 3}\n"
     ]
    }
   ],
   "source": [
    "# Define a tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke with math question\n",
    "print(\"\\n[Example: Tool Binding]\")\n",
    "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
    "\n",
    "# Check if LLM made a tool call\n",
    "if msg.tool_calls:\n",
    "    print(f\"LLM decided to use tool: {msg.tool_calls[0]['name']}\")\n",
    "    print(f\"With arguments: {msg.tool_calls[0]['args']}\")\n",
    "else:\n",
    "    print(f\"LLM responded directly: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04262087-fd6b-44a7-ad1c-09987760e5d9",
   "metadata": {},
   "source": [
    "## PATTERN 1: PROMPT CHAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911835c9-2f76-492e-a5af-7bafd9139676",
   "metadata": {},
   "source": [
    "    PROMPT CHAINING: Sequential LLM calls where each call builds on the previous.\n",
    "    \n",
    "    Use Case: Multi-step refinement (draft â†’ improve â†’ polish)\n",
    "    \n",
    "    Flow:\n",
    "        Generate â†’ Check Quality â†’ Improve â†’ Polish â†’ Done\n",
    "             â”‚            â”‚           â”‚          â”‚\n",
    "             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  Each step uses previous output\n",
    "    \n",
    "    Real-World Examples:\n",
    "    - Content creation: Draft â†’ SEO optimize â†’ Fact-check â†’ Publish\n",
    "    - Code generation: Scaffold â†’ Implement â†’ Test â†’ Document\n",
    "    - Email writing: Draft â†’ Tone adjustment â†’ Grammar check â†’ Send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d80823e-2d0a-47e0-aae1-6db7d81af432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacd776-c79d-4d8c-bf8f-7b3f29bd555c",
   "metadata": {},
   "source": [
    "### Define State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f550e5d-dfc4-45e4-99ea-f557a404c9c6",
   "metadata": {},
   "source": [
    "        State tracks the joke as it progresses through refinement steps.\n",
    "        \n",
    "        Visual State Evolution:\n",
    "        Step 1: {topic: \"cats\", joke: \"Why did cat cross road? To get to other side\"}\n",
    "        Step 2: {topic: \"cats\", joke: \"...\", improved_joke: \"Why don't cats play poker? Too many cheetahs!\"}\n",
    "        Step 3: {topic: \"cats\", joke: \"...\", improved_joke: \"...\", final_joke: \"Why don't cats play poker in the jungle? Too many cheetahs and the stakes are too high!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8073d99d-4616-4472-a00e-1959449dde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7631a7c-a9a1-466f-af84-43e64c12c4df",
   "metadata": {},
   "source": [
    "### Define Nodes (Each step in the chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a32f97-ff84-49de-ad66-60070c98e6d5",
   "metadata": {},
   "source": [
    "    STEP 1: Generate initial joke.\n",
    "    \n",
    "    This is the first LLM call in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e41fb22-2c8e-47ed-984f-b6312ad21cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joke(state: State):\n",
    "    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85caf7-310a-4912-b837-4773b4d0e697",
   "metadata": {},
   "source": [
    "    QUALITY GATE: Check if joke has proper structure.\n",
    "    \n",
    "    This is a CONDITIONAL NODE - it decides which path to take.\n",
    "    - If joke has punctuation (?, !) â†’ Pass (good enough)\n",
    "    - If joke lacks punctuation â†’ Fail (needs improvement)\n",
    "    \n",
    "    Visual:\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚check_punchlineâ”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚                       â”‚\n",
    "            â–¼                       â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚  Pass   â”‚             â”‚  Fail   â”‚\n",
    "      â”‚  â†’ END  â”‚             â”‚  â†’ Fix  â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62476e0e-2c04-461b-b368-19da9d9ce37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_punchline(state: State):\n",
    "    # Simple check - does the joke contain \"?\" or \"!\"\n",
    "    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n",
    "        return \"Pass\"\n",
    "    return \"Fail\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012550fd-3846-474a-9f00-97f1a403d74e",
   "metadata": {},
   "source": [
    "    STEP 2: Improve the joke (only if it failed the check).\n",
    "    \n",
    "    This call uses the OUTPUT of step 1 as INPUT.\n",
    "    That's the \"chaining\" part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296c3564-7942-4b3c-8f06-26fc40c7afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_joke(state: State):\n",
    "    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n",
    "    return {\"improved_joke\": msg.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c5dbf-9760-42e4-8434-b8e8febb450c",
   "metadata": {},
   "source": [
    "    STEP 3: Final polish.\n",
    "    \n",
    "    This call uses the OUTPUT of step 2 as INPUT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "056a59a7-4063-4aed-a4f2-4a269e0c5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polish_joke(state: State):\n",
    "    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n",
    "    return {\"final_joke\": msg.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a0672-6f8a-4446-babd-1874cf1e2be8",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7415d00-8210-4c30-8b52-9ecc67062dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Graph Visualization]\n",
      "START â†’ generate_joke â†’ check â†’ [Pass â†’ END | Fail â†’ improve â†’ polish â†’ END]\n",
      "\n",
      "[Running Prompt Chain]\n",
      "\n",
      "Initial joke:\n",
      "<think>\n",
      "Okay, the user wants a short joke about cats. Let me think about common cat-related humor. Cats are known for their independence, knocking things over, and their love for boxes. Maybe play on those traits.\n",
      "\n",
      "Hmm, maybe something about them being mischievous. Like how they knock things over. Wait, there's a classic joke about cats and the internet. Oh right, the \"why did the cat...\" format. Let me try that.\n",
      "\n",
      "\"Why did the cat bring a ladder to the party?\" Maybe the punchline is about reaching the tuna? Wait, cats love tuna. But maybe something about climbing. Wait, the classic one is \"Why did the cat fall off the roof? Because it was a cat and the roof was a cat.\" No, that's not right. Let me think again.\n",
      "\n",
      "Alternatively, maybe a pun on \"climbing.\" Like, \"Why did the cat bring a ladder to the party? To reach the tuna.\" Wait, that's similar to the existing jokes. Maybe I need a different angle. What about cats and their obsession with boxes? \"Why did the cat bring a box to the party? Because it heard the food was in a box.\" Not great.\n",
      "\n",
      "Wait, maybe something about cats and their ability to open doors. \"Why did the cat get kicked out of the house? Because it was a door.\" No, that's not funny. Let me think of a different approach. Maybe a play on words with \"meow.\" \"Why did the cat cross the road? To get to the other meow.\" That's a twist on the classic \"Why did the chicken cross the road?\" Maybe that's a good direction.\n",
      "\n",
      "Alternatively, \"Why did the cat bring a ladder to the party? To reach the tuna.\" Wait, that's similar to what I thought before. Maybe that's acceptable. Let me check if that's a known joke. I think there's a version of that. Maybe I can tweak it. \"Why did the cat bring a ladder to the party? Because it heard the tuna was on the ceiling.\" That's better. Or maybe \"Why did the cat bring a ladder to the party? To get to the tuna on the top shelf.\" Hmm.\n",
      "\n",
      "Alternatively, maybe a joke about cats and their independence. \"Why did the cat refuse to play with the dog? Because it didn't want to be a pet.\" No, that's not funny. Maybe something about cats and their curiosity. \"Why did the cat go to the doctor? Because it had a purr-fectly good reason to be sick.\" Not great.\n",
      "\n",
      "Wait, maybe the classic \"Why did the cat fall off the table? Because it was a cat and the table was a cat.\" No, that's not making sense. Let me think again. Maybe a play on \"climbing\" and \"cat.\" \"Why did the cat climb the tree? To get to the top of the tree.\" Not funny. Maybe \"Why did the cat bring a ladder to the party? To reach the tuna.\" Maybe that's okay. Let me go with that. It's a simple pun, but it's a common joke structure. Alternatively, maybe a better twist. \"Why did the cat bring a ladder to the party? Because it heard the tuna was on the ceiling.\" That's better. I think that's a solid joke. Let me check for clarity and humor. Yeah, it's a play on the classic joke structure with a pun on tuna and ladder. I think that works.\n",
      "</think>\n",
      "\n",
      "Why did the cat bring a ladder to the party?  \n",
      "To reach the tunaâ€”*it heard the food was on the ceiling!* ğŸ¾\n",
      "\n",
      "--- Passed quality check immediately ---\n"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"generate_joke\", generate_joke)\n",
    "workflow.add_node(\"improve_joke\", improve_joke)\n",
    "workflow.add_node(\"polish_joke\", polish_joke)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"generate_joke\")\n",
    "\n",
    "# Conditional edge based on quality check\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_joke\",\n",
    "    check_punchline,  # Function that returns \"Pass\" or \"Fail\"\n",
    "    {\n",
    "        \"Fail\": \"improve_joke\",  # If fails â†’ improve it\n",
    "        \"Pass\": END              # If passes â†’ done!\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"improve_joke\", \"polish_joke\")\n",
    "workflow.add_edge(\"polish_joke\", END)\n",
    "\n",
    "# Compile\n",
    "chain = workflow.compile()\n",
    "\n",
    "print(\"\\n[Graph Visualization]\")\n",
    "print(\"START â†’ generate_joke â†’ check â†’ [Pass â†’ END | Fail â†’ improve â†’ polish â†’ END]\")\n",
    "\n",
    "# Run it!\n",
    "print(\"\\n[Running Prompt Chain]\")\n",
    "state = chain.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "print(\"\\nInitial joke:\")\n",
    "print(state[\"joke\"])\n",
    "\n",
    "if \"improved_joke\" in state:\n",
    "    print(\"\\n--- Needed improvement ---\")\n",
    "    print(\"\\nImproved joke:\")\n",
    "    print(state[\"improved_joke\"])\n",
    "    print(\"\\nFinal joke:\")\n",
    "    print(state[\"final_joke\"])\n",
    "else:\n",
    "    print(\"\\n--- Passed quality check immediately ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700c63f-6514-4585-a996-56f4db9d54f6",
   "metadata": {},
   "source": [
    "## PATTERN 2: PARALLELIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c93c3-f40e-42a2-9d0f-c83cbbc3bb0c",
   "metadata": {},
   "source": [
    "    PARALLELIZATION: Run multiple LLM calls simultaneously.\n",
    "    \n",
    "    Use Case: Independent tasks that don't depend on each other\n",
    "    \n",
    "    Flow:\n",
    "                  START\n",
    "                    â”‚\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚       â”‚       â”‚\n",
    "            â–¼       â–¼       â–¼\n",
    "          LLM_1   LLM_2   LLM_3  â† All run at same time!\n",
    "            â”‚       â”‚       â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "              Aggregator\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "                  END\n",
    "    \n",
    "    Performance:\n",
    "    - Sequential: 3 LLM calls Ã— 2 seconds each = 6 seconds total\n",
    "    - Parallel:   3 LLM calls running simultaneously = 2 seconds total\n",
    "    \n",
    "    Real-World Examples:\n",
    "    - Social media: Generate post + image caption + hashtags (parallel)\n",
    "    - Research: Summarize multiple documents simultaneously\n",
    "    - Content: Write blog post intro + body + conclusion in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916974c8-4723-49c5-b39e-bfb7ed274683",
   "metadata": {},
   "source": [
    "### Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff25371a-8dc8-440e-b167-297a39d717da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"State for parallel execution\"\"\"\n",
    "    topic: str\n",
    "    joke: str\n",
    "    story: str\n",
    "    poem: str\n",
    "    combined_output: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ea400-72a5-4532-8cd0-773844029e2d",
   "metadata": {},
   "source": [
    "### Define Parallel Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05fca681-f407-47f8-8ae5-54e1b71d826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_1(state: State):\n",
    "    \"\"\"Worker 1: Generate joke\"\"\"\n",
    "    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "def call_llm_2(state: State):\n",
    "    \"\"\"Worker 2: Generate story\"\"\"\n",
    "    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n",
    "    return {\"story\": msg.content}\n",
    "\n",
    "\n",
    "def call_llm_3(state: State):\n",
    "    \"\"\"Worker 3: Generate poem\"\"\"\n",
    "    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n",
    "    return {\"poem\": msg.content}\n",
    "\n",
    "\n",
    "def aggregator(state: State):\n",
    "    \"\"\"\n",
    "    Combine results from all parallel workers.\n",
    "    \n",
    "    This node runs AFTER all parallel workers complete.\n",
    "    LangGraph automatically waits for all parallel branches.\n",
    "    \"\"\"\n",
    "    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n",
    "    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n",
    "    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n",
    "    combined += f\"POEM:\\n{state['poem']}\"\n",
    "    return {\"combined_output\": combined}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bda24-7420-445c-ae58-8d8e5f74811a",
   "metadata": {},
   "source": [
    "### Build Parallel Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18055702-0c88-4ac4-819f-9713332cb2ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Running Parallel Workflow]\n",
      "Here's a story, joke, and poem about cats!\n",
      "\n",
      "STORY:\n",
      "<think>\n",
      "Okay, the user wants a story about cats. Let me think about how to approach this. First, I need to decide on the genre. Maybe a fantasy or magical realism story since cats often have a mystical side. Let me go with magical realism. \n",
      "\n",
      "I should create a setting. A small village sounds cozy. Maybe a village where cats have some special abilities. Wait, the user might want something unique. How about a village where cats are guardians of something? Maybe a hidden realm? \n",
      "\n",
      "Characters: A young cat protagonist. Let's name her Luna. She's curious and adventurous. Maybe she discovers her heritage. Her grandmother could be a wise figure, like a guardian. \n",
      "\n",
      "Conflict: Luna finds an ancient book that reveals her family's role as guardians. She needs to protect the village from a threat. Maybe a curse or a dark force. \n",
      "\n",
      "Plot points: Luna finds the book, learns about her destiny, faces challenges, and ultimately saves the day. Include some magical elements, like the cats having powers or the village being connected to a mystical place. \n",
      "\n",
      "Themes: Heritage, courage, responsibility. Make sure the story has a heartwarming ending. Also, add some details about the cats' abilities, like glowing eyes or telepathy. \n",
      "\n",
      "Wait, the user might want a story that's engaging for all ages. Keep the language simple but vivid. Maybe include some dialogue between Luna and her grandmother. \n",
      "\n",
      "Let me outline the story: Luna discovers the book, learns about her ancestors, faces a threat (maybe a shadow creature), uses her powers to save the village. The grandmother guides her, and in the end, Luna becomes a guardian. \n",
      "\n",
      "Check for flow and make sure each part connects. Add some descriptive elements to make the world come alive. Avoid clichÃ©s but keep it relatable. Okay, time to put it all together into a cohesive story.\n",
      "</think>\n",
      "\n",
      "**Title: The Guardian of Moonlit Hollow**\n",
      "\n",
      "In the quiet village of Moonlit Hollow, where the cobblestone streets whispered secrets and the moonlight danced like silver dust, cats were more than mere pets. They were guardians, keepers of an ancient magic passed down through generations. The villagers revered them, for their eyes held the glow of starlight, and their paws could trace constellations in the air. But among them, one kitten stirred with a destiny yet to unfold.\n",
      "\n",
      "Luna, a sleek black cat with fur like midnight and eyes that shimmered like twin moons, was the youngest of the Hollowâ€™s feline guardians. While her peers patrolled the village, sniffing for shadows or mending fences with their claws, Luna spent her days exploring the woods beyond the village, chasing fireflies and dreaming of the stars. Her grandmother, Elder Mira, often chided her, â€œCuriosity is a gift, little shadow, but remember: the moonâ€™s light is strongest when the world is darkest.â€\n",
      "\n",
      "One twilight, as Luna prowled the edge of the Whispering Woods, she stumbled upon a weathered book half-buried in the moss. Its cover, embossed with a crescent moon, pulsed faintly. Inside, pages fluttered like wings, revealing tales of the Hollowâ€™s foundersâ€”cats who once sealed away a primordial darkness known as the Veil. The final chapter, however, was blankâ€¦ until Lunaâ€™s paw pressed against it, and the words materialized: *â€œWhen the Veil stirs, the Guardian of the Moon shall rise. Only the heart that sees beyond the dark may bind it again.â€*\n",
      "\n",
      "Elder Mira appeared, her silver fur glowing like moonlight. â€œYouâ€™ve awakened the prophecy, Luna. The Veil has begun to awaken. Your ancestorsâ€™ magic flows through you, but it requires courage to wield.â€ She placed a paw on Lunaâ€™s head. â€œThe Veil feeds on fear. To stop it, you must face itâ€”not with claws, but with light.â€\n",
      "\n",
      "The next night, the village trembled. Shadows slithered from the woods, coiling around homes like smoke. Lunaâ€™s heart raced as she leapt into the chaos, her eyes blazing with the power of the moon. The Veil took formâ€”a swirling mass of darkness with eyes like voids, whispering lies to the villagers. â€œYou are powerless,â€ it hissed. â€œYour magic is but a flicker.â€\n",
      "\n",
      "But Luna remembered Miraâ€™s words. She closed her eyes, letting the fear drain from her, and opened her heart. The moonlight around her intensified, weaving into a radiant net that trapped the Veil. With a cry that echoed the stars, Luna chanted the ancient incantation, her voice a melody of hope. The darkness shrieked, unraveling into stardust, and the village was saved.\n",
      "\n",
      "In the dawnâ€™s light, Luna stood atop the village bell tower, her fur aglow. The villagers cheered, but she gazed at the horizon, where the woods met the sky. â€œThe Veil may sleep,â€ she meowed, â€œbut guardians are always needed. Until the next shadow rises, Iâ€™ll watch over this place.â€\n",
      "\n",
      "And so, Luna became the Guardian of Moonlit Hollow, her story etched into the stars, a reminder that even the smallest light can banish the darkest night.\n",
      "\n",
      "JOKE:\n",
      "<think>\n",
      "Okay, the user wants a joke about cats. Let me think about common cat-related humor. Cats are known for their independence, knocking things over, and their love for boxes. Maybe play on those traits.\n",
      "\n",
      "Hmm, what's a typical situation where a cat causes a funny problem? Maybe something with their curiosity leading to a mishap. Like knocking over something important. Maybe a keyboard? Because that's relatable.\n",
      "\n",
      "So, the setup could be about a cat knocking over a keyboard. Then the punchline could involve the cat's reaction. Wait, maybe the cat thinks it's playing a game? Like, the cat knocks over the keyboard, and then the owner is frustrated, but the cat is happy because it thinks it's interacting with the computer. \n",
      "\n",
      "Wait, maybe the joke could be that the cat is trying to \"play\" with the keyboard, but the owner is upset. Or maybe the cat thinks it's typing. Let me structure it. \n",
      "\n",
      "First line: \"Why did the cat get kicked out of the computer lab?\" Then the punchline: \"Because it kept knocking over the keyboard and pretending it was playing a game.\" Wait, maybe not. Let me think again.\n",
      "\n",
      "Alternatively, \"Why don't cats ever get cold?\" \"Because they always have their own little box to hibernate in!\" No, that's more about boxes. Maybe the keyboard idea is better. \n",
      "\n",
      "Wait, here's an idea: \"Why did the cat bring a ladder to the keyboard?\" \"Because it heard the keyboard was a bit low.\" No, that's not funny. Maybe the cat thinks it's typing. \n",
      "\n",
      "Wait, maybe: \"Why did the cat get a job at the computer store?\" \"Because it had a knack for knocking over keyboards and pretending it was typing.\" Hmm, not sure. \n",
      "\n",
      "Alternatively, a play on words. \"Why did the cat cross the road?\" \"To get to the other keyboard!\" No, that's not great. \n",
      "\n",
      "Wait, maybe the classic \"Why did the cat...\" structure. Let me think of a better setup. Maybe the cat is trying to \"play\" with the computer. \n",
      "\n",
      "\"Why did the cat get a ticket for speeding?\" \"Because it was chasing its tail at 80 miles per hour!\" No, that's more about tail chasing. \n",
      "\n",
      "Wait, maybe the joke is about the cat's behavior. Like, \"Why don't cats ever get lost?\" \"Because they always follow the trail of their own tail!\" Not sure. \n",
      "\n",
      "Alternatively, think about the cat's love for boxes. \"Why did the cat bring a suitcase to the beach?\" \"Because it heard the sand was a great place to dig a box.\" Not funny. \n",
      "\n",
      "Back to the keyboard idea. Maybe: \"Why did the cat get kicked out of the computer lab?\" \"Because it kept knocking over the keyboard and pretending it was playing a game.\" Maybe that's okay. Or maybe the punchline is that the cat thinks it's typing. \n",
      "\n",
      "Wait, here's a better one: \"Why don't cats ever get cold? Because they always have their own little box to hibernate in!\" Wait, that's similar to what I thought before. Maybe that's a classic joke. \n",
      "\n",
      "Alternatively, \"Why did the cat bring a ladder to the keyboard? Because it heard the keyboard was a bit low.\" Not great. \n",
      "\n",
      "Wait, maybe the joke is about the cat's curiosity. \"Why did the cat knock over the lamp? Because it was trying to find the switch.\" No. \n",
      "\n",
      "Hmm, maybe I should go with the keyboard joke. Let me try again. \n",
      "\n",
      "\"Why did the cat get a job at the computer store?\" \"Because it had a knack for knocking over keyboards and pretending it was typing.\" Maybe that's okay. \n",
      "\n",
      "Alternatively, \"Why did the cat get kicked out of the computer lab? Because it kept knocking over the keyboard and pretending it was playing a game.\" \n",
      "\n",
      "I think that's a possible joke. Let me check if it's original. Maybe not, but it's a common structure. Alternatively, maybe a pun on \"claw\" and \"clawing.\" \n",
      "\n",
      "Wait, here's another angle: \"Why did the cat bring a ladder to the keyboard? Because it heard the keyboard was a bit low.\" Not funny. \n",
      "\n",
      "Alternatively, \"Why don't cats ever get cold? Because they always have their own little box to hibernate in!\" Maybe that's better. \n",
      "\n",
      "I think I'll go with the keyboard joke. Let me structure it properly. \n",
      "\n",
      "\"Why did the cat get kicked out of the computer lab? Because it kept knocking over the keyboard and pretending it was playing a game.\" \n",
      "\n",
      "Wait, maybe the punchline could be more clever. Like, \"Because it was trying to 'type' with its paws!\" \n",
      "\n",
      "So, \"Why did the cat get kicked out of the computer lab? Because it kept knocking over the keyboard and pretending it was typing with its paws!\" \n",
      "\n",
      "That's better. Or maybe \"Because it kept knocking over the keyboard and the computer thought it was a virus!\" \n",
      "\n",
      "Hmm, not sure. Maybe the first version is better. Let me check if that's a known joke. If not, then it's okay. \n",
      "\n",
      "Alternatively, \"Why did the cat bring a ladder to the keyboard? Because it heard the keyboard was a bit low.\" \n",
      "\n",
      "No, not funny. \n",
      "\n",
      "I think the keyboard joke is better. Let me finalize that.\n",
      "</think>\n",
      "\n",
      "Why did the cat get kicked out of the computer lab?  \n",
      "Because it kept knocking over the keyboard and pretending it was typing with its paws! ğŸ¾ğŸ’»\n",
      "\n",
      "POEM:\n",
      "<think>\n",
      "Okay, the user wants a poem about cats. Let me start by brainstorming some cat-related themes. Cats are mysterious, graceful, and have that independent streak. Maybe I can highlight their agility, their curious nature, and their aloofness.\n",
      "\n",
      "I should think about the structure. Maybe a traditional rhyme scheme would work well, like ABAB or AABB. Let me try quatrains with a consistent meter. Maybe iambic tetrameter? That's common in poetry and flows nicely.\n",
      "\n",
      "First stanza: Introduce cats as silent hunters. Use imagery like midnight, shadows, paws, and whiskers. Maybe something about their eyes glowing in the dark.\n",
      "\n",
      "Second stanza: Their movementsâ€”graceful, like whispers. Maybe mention their playfulness, like chasing toys or batting at light.\n",
      "\n",
      "Third stanza: Their independence. How they have their own rules, maybe sitting on windowsills, watching the world. Contrast their aloofness with their affection when they choose to show it.\n",
      "\n",
      "Fourth stanza: Their nocturnal habits. How they're active at night, maybe comparing them to shadows or stars. Mention their purring as a comforting sound.\n",
      "\n",
      "Fifth stanza: Their enigmatic nature. They can be both playful and mysterious, with secrets of their own. Maybe something about their silent communication and the bond with humans.\n",
      "\n",
      "Conclusion: Wrap it up by celebrating their dualityâ€”both wild and domestic, mysterious yet beloved. End with a tribute to their presence in our lives.\n",
      "\n",
      "Now, check for flow and rhyme. Make sure each stanza transitions smoothly. Use vivid imagery and avoid clichÃ©s. Maybe add some metaphors, like comparing their eyes to embers or their movements to poetry.\n",
      "\n",
      "Let me draft each stanza step by step, ensuring the rhythm is consistent. Read it aloud to check the cadence. Adjust any lines that feel forced. Make sure the poem captures the essence of cats' charm and mystery.\n",
      "</think>\n",
      "\n",
      "**Ode to the Cat**  \n",
      "\n",
      "In midnightâ€™s hush, where shadows creep,  \n",
      "They stalk the dark with silent feet,  \n",
      "Their eyes aglow, a ghostly gleam,  \n",
      "A whisper of the wildâ€™s refrain.  \n",
      "\n",
      "They dance through air with lithe command,  \n",
      "A flick of tail, a pawâ€™s soft spanâ€”  \n",
      "A ball of fur, a fleeting storm,  \n",
      "A poetâ€™s verse in motionâ€™s form.  \n",
      "\n",
      "They reign in solitude, yet share  \n",
      "A glance that binds the heart to careâ€”  \n",
      "A purr, a nudge, a sunlit sprawl,  \n",
      "A kingdom built on fleeting sprawl.  \n",
      "\n",
      "By moonlit windows, they take flight,  \n",
      "A shadowâ€™s dance, a starry night,  \n",
      "Their voices low, a lullaby  \n",
      "That hums the world to gentle sigh.  \n",
      "\n",
      "Mysterious, yet ever near,  \n",
      "They guard the edges of our sphereâ€”  \n",
      "A riddle wrapped in velvet grace,  \n",
      "A silent song the soul can trace.  \n",
      "\n",
      "So hereâ€™s to cats, both wild and tame,  \n",
      "Who weave their magic through our nameâ€”  \n",
      "In every blink, a universe,  \n",
      "A dance of shadows, love, and verse.\n"
     ]
    }
   ],
   "source": [
    "parallel_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n",
    "parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n",
    "parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n",
    "parallel_builder.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# CRITICAL: Multiple edges from START mean PARALLEL execution\n",
    "parallel_builder.add_edge(START, \"call_llm_1\")\n",
    "parallel_builder.add_edge(START, \"call_llm_2\")\n",
    "parallel_builder.add_edge(START, \"call_llm_3\")\n",
    "\n",
    "# All workers feed into aggregator\n",
    "parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n",
    "\n",
    "parallel_builder.add_edge(\"aggregator\", END)\n",
    "\n",
    "# Compile\n",
    "parallel_workflow = parallel_builder.compile()\n",
    "\n",
    "print(\"\\n[Running Parallel Workflow]\")\n",
    "state = parallel_workflow.invoke({\"topic\": \"cats\"})\n",
    "print(state[\"combined_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d08d5b-d8ae-4912-95b7-d0190bf3266b",
   "metadata": {},
   "source": [
    "## PATTERN 3: ROUTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9609778-b582-4295-8fcf-bfce02551511",
   "metadata": {},
   "source": [
    "    ROUTING: Dynamic path selection based on input.\n",
    "    \n",
    "    Use Case: Different inputs need different handling\n",
    "    \n",
    "    Flow:\n",
    "             START\n",
    "               â”‚\n",
    "               â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚ Router  â”‚ â† LLM decides which path\n",
    "          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚      â”‚      â”‚\n",
    "        â–¼      â–¼      â–¼\n",
    "      Story  Joke  Poem  â† Only ONE executes\n",
    "        â”‚      â”‚      â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "             END\n",
    "    \n",
    "    Comparison with Parallelization:\n",
    "    - Parallel: Run ALL branches\n",
    "    - Routing: Run ONE branch (chosen dynamically)\n",
    "    \n",
    "    Real-World Examples:\n",
    "    - Customer support: Route to FAQ / human / chatbot based on query\n",
    "    - Content moderation: Route to auto-approve / review / reject\n",
    "    - Email triage: Route to urgent / normal / spam based on content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b454610e-5bfe-4ae6-b5d9-cba5ff794796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aff7d0-ae17-4b82-abe8-ac23a0df9403",
   "metadata": {},
   "source": [
    "### Define Routing Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf43b9b-ebb1-4d15-98bc-593a2e96e992",
   "metadata": {},
   "source": [
    "    Structured output for routing decision.\n",
    "    \n",
    "    The LLM returns this to tell us which path to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca49a94b-1b42-4adf-a2c2-bb1cc9e06935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Route(BaseModel):\n",
    "    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n",
    "        None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "# Create router (LLM that returns Route)\n",
    "router = llm.with_structured_output(Route)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f640e2-48cd-409d-9bbd-e641eb14e623",
   "metadata": {},
   "source": [
    "### Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "737e76ff-01cb-46ad-bb34-b32fb0fe55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    input: str      # User's request\n",
    "    decision: str   # Router's decision\n",
    "    output: str     # Final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6a397-cfd3-404c-8296-f22d7461fc1e",
   "metadata": {},
   "source": [
    "### Define Nodes (One for each path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08eea59a-9df1-423b-a470-bed8e54eb4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_1(state: State):\n",
    "    \"\"\"Path 1: Write a story\"\"\"\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_2(state: State):\n",
    "    \"\"\"Path 2: Write a joke\"\"\"\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_3(state: State):\n",
    "    \"\"\"Path 3: Write a poem\"\"\"\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_router(state: State):\n",
    "    \"\"\"\n",
    "    Router node: Decides which path to take.\n",
    "    \n",
    "    Uses structured output to make routing decision.\n",
    "    \"\"\"\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Route the input to story, joke, or poem based on the user's request.\"\n",
    "            ),\n",
    "            HumanMessage(content=state[\"input\"]),\n",
    "        ]\n",
    "    )\n",
    "    return {\"decision\": decision.step}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5c751-0ea9-4995-b151-409fa8f8f166",
   "metadata": {},
   "source": [
    "### Define Routing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339bae5-c6b7-4539-ac86-cab11c94f059",
   "metadata": {},
   "source": [
    "    Conditional edge function: Maps decision to node name.\n",
    "    \n",
    "    This function returns the NAME of the next node to visit.\n",
    "    \n",
    "    Flow:\n",
    "        decision=\"story\" â†’ return \"llm_call_1\"\n",
    "        decision=\"joke\"  â†’ return \"llm_call_2\"\n",
    "        decision=\"poem\"  â†’ return \"llm_call_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d2957a3-c4dc-4fed-988d-a87d58c18ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_decision(state: State):\n",
    "    if state[\"decision\"] == \"story\":\n",
    "        return \"llm_call_1\"\n",
    "    elif state[\"decision\"] == \"joke\":\n",
    "        return \"llm_call_2\"\n",
    "    elif state[\"decision\"] == \"poem\":\n",
    "        return \"llm_call_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd916f43-0cf2-4e98-b46b-dc63857257ca",
   "metadata": {},
   "source": [
    "### Build Routing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a249331-35e1-466c-9a5e-8aa0881fb23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Running Routing Workflow]\n",
      "Router decision: joke\n",
      "Output: <think>\n",
      "Okay, the user wants a joke about cats. Let me think about common cat-related humor. Cats ar...\n"
     ]
    }
   ],
   "source": [
    "router_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "router_builder.add_node(\"llm_call_1\", llm_call_1)\n",
    "router_builder.add_node(\"llm_call_2\", llm_call_2)\n",
    "router_builder.add_node(\"llm_call_3\", llm_call_3)\n",
    "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
    "\n",
    "# Add edges\n",
    "router_builder.add_edge(START, \"llm_call_router\")\n",
    "\n",
    "# Conditional routing\n",
    "router_builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,  # Function that returns node name\n",
    "    {\n",
    "        # Map of: return value â†’ node name\n",
    "        \"llm_call_1\": \"llm_call_1\",\n",
    "        \"llm_call_2\": \"llm_call_2\",\n",
    "        \"llm_call_3\": \"llm_call_3\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# All paths lead to END\n",
    "router_builder.add_edge(\"llm_call_1\", END)\n",
    "router_builder.add_edge(\"llm_call_2\", END)\n",
    "router_builder.add_edge(\"llm_call_3\", END)\n",
    "\n",
    "# Compile\n",
    "router_workflow = router_builder.compile()\n",
    "\n",
    "print(\"\\n[Running Routing Workflow]\")\n",
    "state = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\n",
    "print(f\"Router decision: {state.get('decision', 'N/A')}\")\n",
    "print(f\"Output: {state['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cccf70-38d3-4474-a35c-4b3e2c7e9941",
   "metadata": {},
   "source": [
    "## PATTERN 4: ORCHESTRATOR-WORKER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7c20b-0cb0-4750-a20e-a6222c31ed46",
   "metadata": {},
   "source": [
    "    ORCHESTRATOR-WORKER: One LLM plans, many LLMs execute.\n",
    "    \n",
    "    Use Case: Divide large task into subtasks, execute in parallel\n",
    "    \n",
    "    Flow:\n",
    "               START\n",
    "                 â”‚\n",
    "                 â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚ Orchestrator â”‚ â† Creates plan: [task1, task2, task3]\n",
    "          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚      â”‚      â”‚\n",
    "          â–¼      â–¼      â–¼\n",
    "       Worker Worker Worker  â† Each handles one task (parallel)\n",
    "          â”‚      â”‚      â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "                 â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚ Synthesizer  â”‚ â† Combines results\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "                 â–¼\n",
    "               END\n",
    "    \n",
    "    Real-World Examples:\n",
    "    - Report writing: Plan sections â†’ Write each section â†’ Combine\n",
    "    - Research: Break query into sub-questions â†’ Answer each â†’ Synthesize\n",
    "    - Code generation: Plan modules â†’ Generate each â†’ Integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2fe7db6-14d5-47e6-843b-f46537190886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5526d19-e16c-4ac0-ad81-c64751da1a08",
   "metadata": {},
   "source": [
    "### Define Planning Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbb2ad3d-8c74-439b-8361-f2ea481cbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section(BaseModel):\n",
    "    \"\"\"Schema for one section of a report\"\"\"\n",
    "    name: str = Field(description=\"Name for this section of the report.\")\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    \"\"\"Schema for the complete plan\"\"\"\n",
    "    sections: List[Section] = Field(description=\"Sections of the report.\")\n",
    "\n",
    "# Create planner (LLM that returns Sections)\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d31d7e-11e1-45b6-a18f-8feb186fcaa1",
   "metadata": {},
   "source": [
    "### Define States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f4c15-e17d-439e-aadc-164e262e78fe",
   "metadata": {},
   "source": [
    "    Main graph state.\n",
    "    \n",
    "    The Annotated[list, operator.add] is CRITICAL:\n",
    "    - Multiple workers write to completed_sections simultaneously\n",
    "    - operator.add means: APPEND to list (don't replace)\n",
    "    - Without this, workers would overwrite each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "947a7e27-dbdf-4e25-abb6-b66095a739a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    sections: list[Section]\n",
    "    completed_sections: Annotated[list, operator.add]  # Workers append here\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9273add-a9a8-4654-9c1c-bad461839d50",
   "metadata": {},
   "source": [
    "    Individual worker state.\n",
    "    \n",
    "    Each worker gets ONE section to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72eee3e1-6913-4447-962a-149103448541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ab4a5-3fe2-4530-9d10-6201ec5c57da",
   "metadata": {},
   "source": [
    "### Define Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269890d-6a14-4dd4-a4a5-4382865de676",
   "metadata": {},
   "source": [
    "    ORCHESTRATOR: Creates the plan.\n",
    "    \n",
    "    This node:\n",
    "    1. Takes the topic\n",
    "    2. Breaks it into sections\n",
    "    3. Returns list of sections for workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1ce5bc8-61c7-480c-8634-01cbe49d3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State):\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"sections\": report_sections.sections}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1dca6-a5ec-49e6-b16d-915bd9224f54",
   "metadata": {},
   "source": [
    "    WORKER: Writes one section.\n",
    "    \n",
    "    Multiple instances of this node run in parallel.\n",
    "    Each worker gets a different section to write.\n",
    "    \n",
    "    Visual:\n",
    "        Worker 1: section=\"Introduction\"  â†’ writes intro\n",
    "        Worker 2: section=\"Methods\"       â†’ writes methods\n",
    "        Worker 3: section=\"Results\"       â†’ writes results\n",
    "                       (all at same time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd5dd98b-5f8f-4c94-a6d5-33bec1635c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(state: WorkerState):\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # Write to completed_sections (operator.add appends)\n",
    "    return {\"completed_sections\": [section.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2457142-4b40-4a8e-b917-edb1a66d622a",
   "metadata": {},
   "source": [
    "    SYNTHESIZER: Combines all sections.\n",
    "    \n",
    "    This node runs AFTER all workers complete.\n",
    "    It takes all completed sections and combines them into final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a17faa1a-1f85-46d9-ac98-4bc0911bf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesizer(state: State):\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "    return {\"final_report\": completed_report_sections}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c4762-6b66-43d2-ba36-e3dd02e22d51",
   "metadata": {},
   "source": [
    "### Define Worker Assignment Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4567fe1-42eb-4f81-b926-d007630a2d1b",
   "metadata": {},
   "source": [
    "    Create one worker for each section.\n",
    "    \n",
    "    The Send() API is CRITICAL for orchestrator-worker pattern:\n",
    "    - Send(\"node_name\", {\"data\": ...}) creates a parallel execution\n",
    "    - Each Send creates an independent worker\n",
    "    - All workers run simultaneously\n",
    "    \n",
    "    Visual:\n",
    "        sections = [Section1, Section2, Section3]\n",
    "        \n",
    "        Returns: [\n",
    "            Send(\"llm_call\", {\"section\": Section1}),\n",
    "            Send(\"llm_call\", {\"section\": Section2}),\n",
    "            Send(\"llm_call\", {\"section\": Section3})\n",
    "        ]\n",
    "        \n",
    "        Result: 3 parallel workers, each writing one section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8aa82e6-910a-4647-8d88-f0a6fcf832d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "\n",
    "def assign_workers(state: State):\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392b025-4548-473b-8f22-5737581f3c4a",
   "metadata": {},
   "source": [
    "### Build Orchestrator-Worker Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "433d146c-fa13-46e6-a8a1-8c48b280ecf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Running Orchestrator-Worker Pattern]\n",
      "Generated report with 8 sections\n",
      "Final report length: 32820 characters\n"
     ]
    }
   ],
   "source": [
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "\n",
    "# CRITICAL: assign_workers creates dynamic parallel workers\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\",\n",
    "    assign_workers,  # Returns list of Send() objects\n",
    "    [\"llm_call\"]     # Worker node name\n",
    ")\n",
    "\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "print(\"\\n[Running Orchestrator-Worker Pattern]\")\n",
    "state = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n",
    "print(f\"Generated report with {len(state['sections'])} sections\")\n",
    "print(f\"Final report length: {len(state['final_report'])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77bfc87e-6147-445b-8a88-33bf5371bc63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to write a report section called \"Introduction and description\" about LLM scaling laws, their significance in AI, and their impact on developing large language models. Let me start by recalling what LLM scaling laws are. They refer to the observation that as the size of a language model increases, its performance on various tasks improves, often following a power-law relationship. \n",
      "\n",
      "First, I need to define scaling laws clearly. Maybe mention the key studies like the one by Kaplan et al. from 2020. They showed that model performance scales with parameters and data. Then, explain the significance: why this matters for AI. It's about understanding how to optimize resources and predict performance gains. \n",
      "\n",
      "Next, the impact on large language models. I should talk about how scaling laws guide the development of larger models, leading to better capabilities. Also, mention the trade-offs, like computational costs and energy use. Maybe touch on the shift from smaller models to massive ones, and the challenges in training and inference.\n",
      "\n",
      "Wait, the user specified no preamble for each section. So the section should start directly with the content. Use markdown formatting. Let me structure it with headings and bullet points if needed. Make sure to include the significance and impact clearly. Also, check if there are any recent developments or studies that should be mentioned. Maybe the trend towards even larger models and the debate around diminishing returns. \n",
      "\n",
      "I need to ensure the section is concise but comprehensive. Avoid jargon where possible, but since it's a report, some technical terms are okay. Make sure each part flows logically: introduction to scaling laws, their significance, and their impact. Let me verify the key points again to cover all aspects the user asked for.\n",
      "</think>\n",
      "\n",
      "**Introduction and description**  \n",
      "LLM scaling laws describe the empirical relationship between the size of a language model (measured in parameters, data, or compute) and its performance on downstream tasks. These laws reveal that as models grow larger, their capabilities improve non-linearly, often following a power-law trend where performance gains accelerate with increasing scale. This phenomenon has become a cornerstone of modern AI research, enabling the development of increasingly sophisticated large language models (LLMs) capable of handling complex tasks such as natural language understanding, code generation, and reasoning.  \n",
      "\n",
      "The significance of scaling laws lies in their ability to predict performance improvements and guide resource allocation. By understanding how model size, training data, and compute influence outcomes, researchers can optimize training strategies, reduce costs, and accelerate innovation. For instance, studies have shown that doubling the number of parameters or training examples can yield disproportionately larger gains in task accuracy, particularly for challenging benchmarks. This has driven the creation of models with billions or even trillions of parameters, such as GPT-3, PaLM, and others, which demonstrate enhanced generalization and adaptability.  \n",
      "\n",
      "The impact of scaling laws on LLM development is profound. They have shifted the focus from incremental improvements to scaling as the primary pathway for advancing AI capabilities. However, this approach also raises challenges, including computational costs, energy consumption, and the risk of diminishing returns. Despite these hurdles, scaling laws remain a critical framework for pushing the boundaries of what LLMs can achieve, shaping the trajectory of AI research and application.\n",
      "\n",
      "---\n",
      "\n",
      "<think>\n",
      "Okay, the user wants me to write a report section called \"Understanding LLM Scaling Laws\" based on the given description. Let me start by recalling what LLM scaling laws are. They refer to the patterns observed when scaling up large language models. The description mentions parameter count, training data size, and computational resources. I need to define these scaling laws first.\n",
      "\n",
      "I should explain that scaling laws describe how model performance improves with increases in parameters, data, and compute. Then, break down each key concept. For parameter count, I need to mention that more parameters generally lead to better performance, but there's a diminishing returns effect. Maybe include the power law relationship here.\n",
      "\n",
      "Next, training data size. More data can improve performance, but again, there's a point where adding more data doesn't help much. I should note the trade-off between data size and parameter count. Also, mention that data quality matters, not just quantity.\n",
      "\n",
      "Computational resources are another factor. Training larger models requires more compute, which affects cost and time. Maybe touch on how compute is often a limiting factor, especially when scaling up.\n",
      "\n",
      "I need to discuss how these factors influence model performance. Maybe use examples like the scaling laws observed in studies, such as the one by Kaplan et al. 2020. Highlight that while increasing parameters and data helps, there's a point of diminishing returns. Also, computational constraints can limit how much you can scale.\n",
      "\n",
      "Wait, the user said to include no preamble for each section. So the section should start with the heading, then the content. Use markdown formatting. Make sure each key concept is a subsection. Let me structure it with headings for each key concept and then a discussion section.\n",
      "\n",
      "Check if I need to mention specific studies or just general concepts. The description doesn't specify, so maybe keep it general but mention that empirical studies have shown these trends. Also, ensure that the explanation is clear and concise, avoiding jargon where possible. Make sure to connect each factor to model performance, like how more parameters allow the model to capture more patterns, but only up to a certain point.\n",
      "\n",
      "I should also mention that the relationship isn't linear, so the scaling laws are more about the trends rather than exact formulas. Maybe include that the optimal balance between parameters, data, and compute is crucial for efficient model development. Alright, that should cover the main points. Let me put it all together in markdown format without any preamble.\n",
      "</think>\n",
      "\n",
      "# Understanding LLM Scaling Laws  \n",
      "\n",
      "## Key Concepts  \n",
      "**Parameter Count**: The number of parameters in a model determines its capacity to learn complex patterns. Larger models generally exhibit improved performance on downstream tasks, though diminishing returns occur beyond a certain threshold.  \n",
      "\n",
      "**Training Data Size**: The quantity and quality of training data influence a modelâ€™s ability to generalize. Increasing data size often leads to performance gains, but the marginal benefit decreases as data scales.  \n",
      "\n",
      "**Computational Resources**: Training and inference costs scale with model size and data volume. Higher computational resources enable faster training and more efficient inference, but they also increase energy consumption and financial costs.  \n",
      "\n",
      "## Influence on Model Performance  \n",
      "Scaling laws describe empirical trends where model performance improves with increases in parameters, data, and compute, but these improvements follow non-linear relationships. For example, studies show that performance gains from adding parameters diminish as models grow larger, while data size and compute requirements scale superlinearly. Balancing these factors is critical for optimizing efficiency and effectiveness in large language model development.\n",
      "\n",
      "---\n",
      "\n",
      "<think>\n",
      "Okay, the user wants me to write a report section called \"Empirical Evidence of Scaling Laws\" based on the given description. Let me start by understanding what they need. The description mentions presenting empirical studies and results from research papers that show the relationship between model size, data size, and performance metrics like accuracy, perplexity, and task performance.\n",
      "\n",
      "First, I should recall key research papers that have explored scaling laws. The most notable ones are the ones from DeepMind and OpenAI. The DeepMind paper from 2020, \"Scaling Laws for Neural Language Models,\" comes to mind. They showed that increasing model size and data size leads to better performance. Then there's the OpenAI paper from 2020, \"Empirical Model Scaling Laws,\" which also discusses similar findings. More recent studies, like the one from 2023 by Google, might have more data points or different metrics.\n",
      "\n",
      "I need to structure each study with the paper title, authors, year, and key findings. For each, I should mention the model size, data size, and the performance metrics. Also, the user wants markdown formatting, so I'll use headers and bullet points. No preamble, so each section starts with the section name and then the content.\n",
      "\n",
      "Wait, the user specified no preamble for each section. So the section name is \"Empirical Evidence of Scaling Laws,\" and then under that, each study is a subsection. Let me check if I have all the necessary details. For example, the DeepMind study used a 1.5B parameter model and 100GB of data, leading to improvements in perplexity and accuracy. The OpenAI study had a 175B model with 570GB data, showing similar trends. The Google 2023 study might have more parameters and data, showing even better performance.\n",
      "\n",
      "I should also mention the key takeaways from these studies, like the diminishing returns and the importance of both model and data size. Need to make sure the markdown is correct, using bold for the paper titles and authors. Also, check for any recent studies that might have been published after 2023, but since the user didn't specify, sticking to the mentioned ones is safer.\n",
      "\n",
      "Let me verify the details again. The DeepMind paper's title is \"Scaling Laws for Neural Language Models,\" authors are Chinchilla team, 2020. OpenAI's paper is \"Empirical Model Scaling Laws\" by Kaplan et al., 2020. The Google study from 2023 is \"Large-Scale Language Model Training with Sparse Attention,\" by Google Research. Each of these has specific metrics and results. I need to present them clearly, ensuring that each study's contribution is highlighted. Also, mention the performance metrics like perplexity, accuracy, and task performance. Alright, that should cover the user's request.\n",
      "</think>\n",
      "\n",
      "**Empirical Evidence of Scaling Laws**  \n",
      "\n",
      "- **\"Scaling Laws for Neural Language Models\" (DeepMind, 2020)**  \n",
      "  - **Model Size**: 1.5B to 100B parameters.  \n",
      "  - **Data Size**: 100GB to 10TB of text.  \n",
      "  - **Performance Metrics**: Perplexity decreased by 15% with 10x model size increase, while accuracy improved by 8% with 5x data size. Task performance (e.g., GLUE benchmarks) scaled linearly with model size.  \n",
      "\n",
      "- **\"Empirical Model Scaling Laws\" (OpenAI, 2020)**  \n",
      "  - **Model Size**: 175B parameters.  \n",
      "  - **Data Size**: 570GB of text.  \n",
      "  - **Performance Metrics**: Perplexity dropped by 22% with 10x model size, and task performance (e.g., MMLU) improved by 12% with 5x data size. Diminishing returns observed beyond 100B parameters.  \n",
      "\n",
      "- **\"Large-Scale Language Model Training with Sparse Attention\" (Google Research, 2023)**  \n",
      "  - **Model Size**: 1.5T parameters.  \n",
      "  - **Data Size**: 10TB of text.  \n",
      "  - **Performance Metrics**: Perplexity improved by 30% compared to 100B parameter models, and task performance (e.g., code generation) increased by 18% with 10x data size. Scaling efficiency plateaued at 500B parameters.  \n",
      "\n",
      "- **Key Takeaways**:  \n",
      "  - Model and data size exhibit strong positive correlations with performance metrics.  \n",
      "  - Diminishing returns occur at scale, with efficiency gains diminishing beyond certain thresholds.  \n",
      "  - Task-specific performance (e.g., NLP, code, math) scales differently, requiring tailored scaling strategies.\n",
      "\n",
      "---\n",
      "\n",
      "<think>\n",
      "Okay, the user wants me to write a report section called \"Theoretical Foundations of Scaling Laws\" based on the given description. Let me start by understanding what they need. The description mentions exploring the theoretical underpinnings, including the scaling law formula, diminishing returns, and data efficiency in model training.\n",
      "\n",
      "First, I should define scaling laws in the context of machine learning. Maybe start with the basic formula, like the one from the paper by Kaplan et al. The formula is something like F = k * (N^a) * (D^b) * (T^c), where F is the model's capability, N is parameters, D is data, T is training time, and k, a, b, c are constants. I need to explain each variable and the exponents.\n",
      "\n",
      "Next, diminishing returns. As models scale, the marginal gains might decrease. For example, increasing parameters beyond a certain point might not lead to proportional improvements. I should mention how this affects the scaling law formula, maybe the exponents changing or the constants adjusting.\n",
      "\n",
      "Then data efficiency. How does the amount of data affect model performance? Maybe discuss how more data can improve performance but with diminishing returns. Also, the role of data efficiency in training, like how efficient models can use less data to achieve similar results, which ties into the scaling law's D variable.\n",
      "\n",
      "I need to make sure each section is concise and uses markdown. No preamble, so just the section title and content. Let me check if I'm missing any key points. The user might be looking for a clear explanation of the theoretical aspects, so including the formula, diminishing returns, and data efficiency with their implications is important. Also, maybe mention the trade-offs between parameters, data, and training time. Alright, structure each subsection with headers and bullet points or explanations. Make sure it's all in markdown format without any markdown in the thinking process. Let me put it all together now.\n",
      "</think>\n",
      "\n",
      "### Theoretical Foundations of Scaling Laws  \n",
      "\n",
      "**Scaling Law Formula**  \n",
      "The scaling law formula quantifies the relationship between model performance and resource allocation, often expressed as:  \n",
      "$$ F = k \\cdot (N^\\alpha) \\cdot (D^\\beta) \\cdot (T^\\gamma) $$  \n",
      "where $ F $ represents model capability (e.g., accuracy, throughput), $ N $ is the number of parameters, $ D $ is the data volume, $ T $ is training time, and $ k, \\alpha, \\beta, \\gamma $ are empirical constants. This formula captures how increasing parameters, data, or training time proportionally enhances model performance, though the exponents $ \\alpha, \\beta, \\gamma $ reflect diminishing returns and resource efficiency.  \n",
      "\n",
      "**Diminishing Returns**  \n",
      "As models scale, marginal gains in performance decrease due to saturation effects. For instance, increasing parameters beyond a threshold may yield minimal improvements in accuracy, while requiring exponentially more computational resources. This phenomenon is critical in scaling laws, as it implies that the exponents $ \\alpha, \\beta, \\gamma $ are not fixed and may vary with model size, data quality, or task complexity. Diminishing returns also highlight the trade-off between resource allocation and performance gains, guiding efficient scaling strategies.  \n",
      "\n",
      "**Data Efficiency in Model Training**  \n",
      "Data efficiency refers to the ability of a model to achieve high performance with limited data, which is central to scaling laws. Efficient models leverage techniques like transfer learning, data augmentation, or sparse training to reduce the dependency on large datasets. In the context of scaling laws, data efficiency directly impacts the exponent $ \\beta $, as models with higher data efficiency can achieve comparable performance with smaller $ D $, thereby optimizing resource usage. This is particularly relevant in scenarios where data collection is costly or constrained.  \n",
      "\n",
      "**Interplay of Parameters, Data, and Training Time**  \n",
      "The scaling law framework emphasizes the interdependence of parameters, data, and training time. For example, increasing parameters may reduce the required data volume (via better feature representation) or training time (via faster convergence). Conversely, insufficient data can limit parameter utility, while excessive parameters may lead to overfitting without adequate data. Balancing these factors is essential for optimizing model performance under resource constraints.\n",
      "\n",
      "---\n",
      "\n",
      "<think>\n",
      "Okay, the user wants me to write a report section titled \"Challenges and Limitations\" based on the given description. Let me start by understanding the key points they mentioned: computational costs, data requirements, energy consumption, overfitting, and poor generalization.\n",
      "\n",
      "First, I need to structure each of these points into separate subsections. The user specified no preamble, so each section should start directly with the heading. I should use markdown formatting, so I'll make sure to use the appropriate headers and bullet points where necessary.\n",
      "\n",
      "Computational costs: I should mention the need for high-performance hardware like GPUs and TPUs, the complexity of training, and the time required. Maybe include something about distributed computing and the trade-off between model size and efficiency.\n",
      "\n",
      "Data requirements: Here, the volume and quality of data are crucial. I need to talk about the need for large, diverse datasets, the challenges in data curation, and the risk of biases. Also, mention the cost of data collection and labeling.\n",
      "\n",
      "Energy consumption: This ties into computational costs. I should discuss the environmental impact, carbon footprint, and the need for sustainable practices. Maybe touch on green computing initiatives or energy-efficient hardware.\n",
      "\n",
      "Overfitting and poor generalization: Explain how large models might memorize data instead of learning patterns. Mention the risk of performance drops on unseen data and the importance of regularization techniques. Also, the challenge of balancing model complexity with generalization.\n",
      "\n",
      "I should ensure each subsection is concise but covers all the points. Avoid technical jargon where possible, but since it's a report, some terms are necessary. Check that the flow is logical, moving from computational to data to energy, then to model-specific issues. Make sure there's no markdown in the thinking process, just plain text. Alright, time to put it all together in markdown format as specified.\n",
      "</think>\n",
      "\n",
      "### Challenges and Limitations  \n",
      "\n",
      "**Computational Costs**  \n",
      "Scaling large language models (LLMs) demands significant computational resources, including high-performance GPUs/TPUs and distributed computing frameworks. Training these models requires extensive time and financial investment, with costs escalating exponentially as model size increases. The complexity of optimizing hyperparameters and managing distributed training further exacerbates resource demands.  \n",
      "\n",
      "**Data Requirements**  \n",
      "LLMs rely on vast amounts of high-quality, diverse data for training. Curating and annotating such datasets is labor-intensive and expensive, with risks of bias or incomplete representation. Additionally, data privacy and ethical concerns arise when using sensitive or proprietary information, limiting the scope of available training material.  \n",
      "\n",
      "**Energy Consumption**  \n",
      "The energy required to train and deploy large models has a substantial environmental impact, contributing to carbon emissions and resource depletion. Sustainable practices, such as using renewable energy sources or optimizing training efficiency, are critical to mitigate these effects while maintaining scalability.  \n",
      "\n",
      "**Overfitting and Poor Generalization**  \n",
      "LLMs risk overfitting to training data, memorizing specific patterns rather than learning generalizable knowledge. This can lead to poor performance on novel tasks or domains, especially when data distribution shifts. Balancing model complexity with generalization capabilities remains a key challenge, requiring advanced regularization techniques and careful validation strategies.\n",
      "\n",
      "---\n",
      "\n",
      "<think>\n",
      "Okay, the user wants me to write a report section titled \"Case Studies and Real-World Applications\" with the description about scaling laws in NLP, machine translation, and text generation. Let me start by recalling what scaling laws are. They refer to how model performance improves with increased parameters and data, right?\n",
      "\n",
      "First, I need to find real-world examples where scaling laws were applied. The user mentioned NLP, machine translation, and text generation. Let me think of specific models or studies. For NLP, maybe the GPT series by OpenAI? They have research on scaling laws. Then, machine translation could involve models like BERT or Transformer-based systems. Text generation might include models like GPT-3 or others.\n",
      "\n",
      "Wait, the user specified case studies, so I need actual examples. Let me check if there are specific papers or projects. Oh, the Scaling Laws for Neural Language Models paper by OpenAI is a key one. They showed that increasing parameters and data leads to better performance. Then, for machine translation, maybe the work by Google on large-scale models like mT5 or the recent PaLM models. Also, the study by DeepMind on scaling laws in language models could be relevant.\n",
      "\n",
      "I should structure each case study with a title, description, and key findings. Make sure to mention the models, the scaling factors (parameters and data), and the performance improvements. Also, include how these applications improved real-world tasks like translation accuracy or text generation quality.\n",
      "\n",
      "Wait, the user wants markdown formatting. So I need to use headers, bullet points, and maybe bold for key terms. Let me outline each case study. First, the GPT series example. Then, machine translation with mT5 or similar. Third, text generation with models like GPT-3 or others. Also, maybe mention the study by DeepMind on scaling laws in language models as another case study.\n",
      "\n",
      "I need to ensure that each section is concise but informative. Avoid any preamble, just the section as per the user's instruction. Let me verify the details for each case study to make sure they're accurate. For example, the GPT-3 paper by OpenAI in 2020, the scaling laws paper, and the DeepMind study. Also, check if there are specific metrics like BLEU scores for translation or perplexity for text generation.\n",
      "\n",
      "Make sure to highlight the application of scaling laws in each case, how they improved performance, and the real-world impact. Maybe mention specific results, like a 20% improvement in translation accuracy with scaled models. Also, note the trade-offs, like increased computational costs, but the user didn't ask for that, so maybe just focus on the positive outcomes as per the description.\n",
      "\n",
      "Alright, putting it all together in markdown format with the specified sections. Each case study should have a heading, description, and key findings. Use bold for the model names and key terms. Keep the language clear and professional, avoiding markdown in the thinking process but ensuring the final output is properly formatted.\n",
      "</think>\n",
      "\n",
      "### Case Studies and Real-World Applications  \n",
      "\n",
      "**1. GPT Series and Neural Language Models**  \n",
      "OpenAIâ€™s research on scaling laws demonstrated that increasing model parameters and training data significantly improves language understanding tasks. For example, the GPT-3 model, with 175 billion parameters, achieved state-of-the-art results in tasks like question answering, code generation, and text completion. By scaling both parameters and data, OpenAI observed consistent performance gains, validating the hypothesis that larger models outperform smaller ones when trained on extensive datasets.  \n",
      "\n",
      "**2. Machine Translation with mT5 and Transformer Models**  \n",
      "Googleâ€™s mT5 model, a multilingual transformer-based system, leveraged scaling laws to enhance translation accuracy across 100 languages. By increasing the number of parameters and training data, mT5 achieved lower BLEU scores compared to smaller models, demonstrating that scaling improves cross-lingual transfer and contextual understanding. This approach enabled more efficient handling of low-resource languages, reducing the need for language-specific fine-tuning.  \n",
      "\n",
      "**3. Text Generation with PaLM and Large-Scale Pretraining**  \n",
      "Googleâ€™s PaLM (Pathways Language Model) applied scaling laws to text generation, achieving high-quality outputs by training on massive datasets and using billions of parameters. The modelâ€™s ability to generate coherent, contextually relevant text was validated through benchmarks like the MMLU (MMLU) and Winograd Schema tests. Scaling laws here showed that larger models better capture long-range dependencies and nuanced linguistic patterns, leading to more accurate and diverse outputs.  \n",
      "\n",
      "**4. DeepMindâ€™s Scaling Laws in Language Models**  \n",
      "DeepMindâ€™s study on scaling laws revealed that model performance in NLP tasks (e.g., GLUE benchmark) improves exponentially with parameter count and data size. By training models with varying scales, they found that even modest increases in parameters led to significant gains in downstream tasks, such as sentiment analysis and natural language inference. This research underscored the importance of scaling as a strategy for achieving robust, generalizable language models.  \n",
      "\n",
      "**Key Takeaways**  \n",
      "- Scaling laws consistently improve performance in NLP tasks by increasing model size and training data.  \n",
      "- Applications in machine translation and text generation have shown measurable gains in accuracy and efficiency.  \n",
      "- Real-world deployment of scaled models has enabled advancements in multilingual support, code generation, and complex reasoning tasks.\n",
      "\n",
      "---\n",
      "\n",
      "<think>\n",
      "Okay, the user wants me to write a report section titled \"Future Directions and Research Opportunities\" based on the given description. Let me start by understanding the key points they mentioned: efficient training methods, better data utilization, and scalable architectures.\n",
      "\n",
      "First, I need to structure the section properly. The user specified no preamble, so I should jump straight into the content. Using markdown, I'll use headings and bullet points for clarity. \n",
      "\n",
      "For efficient training methods, I should mention techniques like model parallelism and distributed training. Maybe touch on optimization algorithms and hardware acceleration. Also, the user might be interested in how these methods can reduce costs and time.\n",
      "\n",
      "Next, better data utilization. Here, I can discuss data efficiency techniques such as active learning and data augmentation. Also, the importance of high-quality, diverse datasets and maybe synthetic data generation. The user might want to know how these approaches can improve model performance without needing more data.\n",
      "\n",
      "Scalable architectures are another key area. I should talk about modular designs, efficient attention mechanisms, and hybrid models. Maybe mention specific architectures like Mixture of Experts (MoE) and how they allow for scaling. Also, the role of hardware-software co-design in enabling scalability.\n",
      "\n",
      "I need to ensure each subsection is clear and concise. Let me check if I'm covering all the points from the description. Efficient training, data utilization, scalable architecturesâ€”yes, those are the main areas. \n",
      "\n",
      "Wait, the user also mentioned \"advancing the understanding and application of LLM scaling laws.\" Should I include something about scaling laws themselves? Maybe in the scalable architectures section, discussing how scaling laws relate to model size and performance. But the main focus is on the three areas listed. \n",
      "\n",
      "I should avoid any markdown in the thinking process, but the final answer needs to be in markdown. Let me make sure each section is properly formatted with headings and bullet points. Also, use bold for section titles as per the example. \n",
      "\n",
      "Let me review the example response to see the structure. They have three main sections with bold headings, each containing bullet points. I'll follow that format. \n",
      "\n",
      "I need to ensure that the language is professional but clear, avoiding jargon where possible. The user is likely looking for actionable research directions, so the suggestions should be specific and forward-looking. \n",
      "\n",
      "Double-checking the requirements: no preamble, markdown, sections as specified. Alright, I think that's covered. Time to put it all together.\n",
      "</think>\n",
      "\n",
      "**Future Directions and Research Opportunities**  \n",
      "- **Efficient Training Methods**: Investigate advanced optimization techniques, such as sparse training and dynamic computation, to reduce computational overhead while maintaining model performance. Explore hybrid approaches combining model parallelism with distributed training to balance scalability and resource efficiency.  \n",
      "- **Data Utilization**: Focus on improving data efficiency through active learning, data augmentation, and transfer learning strategies. Leverage high-quality, diverse datasets and synthetic data generation to enhance generalization and reduce dependency on massive labeled corpora.  \n",
      "- **Scalable Architectures**: Develop modular, hierarchical architectures that enable seamless scaling of model size and parameter counts. Prioritize research on efficient attention mechanisms, sparse representations, and hybrid models (e.g., Mixture of Experts) to balance performance and resource constraints.  \n",
      "- **Interdisciplinary Integration**: Foster collaboration between NLP, computer vision, and reinforcement learning to create unified frameworks for multi-modal and task-agnostic scaling laws.  \n",
      "- **Ethical and Sustainable Scaling**: Address environmental and societal impacts of large-scale models by designing energy-efficient training pipelines and exploring ethical guidelines for responsible scaling.\n",
      "\n",
      "---\n",
      "\n",
      "<think>\n",
      "Okay, the user wants me to write a conclusion section for their report. Let me start by recalling the key points they mentioned. They want a summary of the report's main ideas, reiterate the importance of scaling laws in LLM development, and highlight future advancements.\n",
      "\n",
      "First, I need to make sure I cover all the elements. The conclusion should wrap up the discussion without introducing new information. Scaling laws are crucial here, so I should explain how increasing model size and data leads to better performance. Maybe mention specific examples like parameter counts and training data volume.\n",
      "\n",
      "Next, the user wants to highlight future potential. I should think about areas like efficiency, specialized models, and ethical considerations. Maybe touch on how scaling laws could lead to more efficient training methods or better handling of specific tasks. Also, ethical aspects like bias and environmental impact are important for future advancements.\n",
      "\n",
      "I need to structure this in markdown. The section name is \"Conclusion,\" so I'll start with that as a heading. Then, a paragraph summarizing the key points. Make sure to reiterate the role of scaling laws, perhaps mention the empirical evidence from studies. Then, the future part should discuss possible directions, maybe mention interdisciplinary approaches or new architectures.\n",
      "\n",
      "Wait, the user said no preamble for each section, so just the section name and content. Let me check if I'm using the right formatting. Use bold for the section name, then the content in paragraphs. Avoid any markdown except for the heading. Also, keep the language concise and professional, as it's a report section.\n",
      "\n",
      "Let me make sure I'm not missing any key points. The main ideas from the report would be the impact of scaling laws on LLM performance, the empirical evidence supporting them, and the implications for future research. Future advancements could include more efficient scaling, specialized models, and addressing ethical issues. That should cover it. Alright, time to put it all together in markdown format.\n",
      "</think>\n",
      "\n",
      "**Conclusion**  \n",
      "The report underscores the transformative role of scaling laws in advancing large language models (LLMs), demonstrating that increased model size, training data, and computational resources consistently correlate with improved performance across diverse tasks. Empirical evidence highlights the diminishing marginal returns of scaling, emphasizing the need for optimized architectures and efficient training strategies. Key insights include the criticality of parameter counts, data diversity, and computational efficiency in achieving state-of-the-art results.  \n",
      "\n",
      "The importance of scaling laws lies in their ability to guide resource allocation and research priorities, enabling the development of models that balance capability with practical constraints. As the field progresses, future advancements may focus on refining scaling strategies to enhance efficiency, reduce environmental impact, and address ethical challenges such as bias and fairness. Innovations in model compression, specialized architectures, and interdisciplinary collaboration are poised to further unlock the potential of LLMs, ensuring their responsible and impactful integration into real-world applications.\n"
     ]
    }
   ],
   "source": [
    "print(state['final_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d42fc1-697d-4981-95fd-0e6264f32e24",
   "metadata": {},
   "source": [
    "## PATTERN 5: EVALUATOR-OPTIMIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5bfbd2-3986-4c52-ad0b-e5d43acaf191",
   "metadata": {},
   "source": [
    "    EVALUATOR-OPTIMIZER: Self-improving loop with quality feedback.\n",
    "    \n",
    "    Use Case: Generate content, evaluate quality, retry if needed\n",
    "    \n",
    "    Flow:\n",
    "             START\n",
    "               â”‚\n",
    "               â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚Generator â”‚ â† Creates content\n",
    "          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚Evaluator â”‚ â† Grades quality\n",
    "          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "          â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    "          â”‚           â”‚\n",
    "          â–¼           â–¼\n",
    "       Accept      Reject\n",
    "        END      (with feedback)\n",
    "                  â”‚\n",
    "                  â””â”€â”€â–º Loop back to Generator\n",
    "    \n",
    "    This creates a LOOP until quality threshold is met.\n",
    "    \n",
    "    Real-World Examples:\n",
    "    - Code generation: Generate â†’ Test â†’ Fix â†’ Repeat until tests pass\n",
    "    - Content creation: Draft â†’ Grade â†’ Revise â†’ Repeat until acceptable\n",
    "    - Translation: Translate â†’ Evaluate accuracy â†’ Retry â†’ Repeat\n",
    "    - Image generation: Generate â†’ Check quality â†’ Regenerate with feedback\n",
    "    \n",
    "    EXECUTION TRACE:\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Iteration 1:\n",
    "      Generator: \"Why did the cat cross the road?\"\n",
    "      Evaluator: Grade = \"not funny\", Feedback = \"Too generic, add wordplay\"\n",
    "      Decision: REJECT â†’ Loop back\n",
    "    \n",
    "    Iteration 2:\n",
    "      Generator (with feedback): \"Why don't cats play poker? Too many cheetahs!\"\n",
    "      Evaluator: Grade = \"funny\", Feedback = \"\"\n",
    "      Decision: ACCEPT â†’ END\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d036747-f523-4fba-abdc-7fb1ee0f3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e9783-750e-4b00-8df8-91cb8ac3980a",
   "metadata": {},
   "source": [
    "### Define State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d192e-f0ed-40ad-aa56-4dbd6d3b03bd",
   "metadata": {},
   "source": [
    "    State tracks content through improvement loop.\n",
    "    \n",
    "    Fields:\n",
    "        topic: What to generate content about\n",
    "        joke: Current version of the joke\n",
    "        feedback: Feedback from evaluator (if rejected)\n",
    "        funny_or_not: Evaluation result (\"funny\" or \"not funny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "771d4344-a46c-4fa3-aaaa-e52e556bb410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    joke: str\n",
    "    topic: str\n",
    "    feedback: str\n",
    "    funny_or_not: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01295d2-d784-4130-b4bf-e20f834a92fe",
   "metadata": {},
   "source": [
    "### Define Evaluation Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b526a-e139-43ae-8a34-76453dc0b20a",
   "metadata": {},
   "source": [
    "    Structured output for evaluation.\n",
    "    \n",
    "    The evaluator LLM returns this to tell us:\n",
    "    - Is the content acceptable?\n",
    "    - If not, how to improve it?\n",
    "    \n",
    "    Example Output:\n",
    "    {\n",
    "        \"grade\": \"not funny\",\n",
    "        \"feedback\": \"The punchline is too predictable. Try adding unexpected wordplay.\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da155c0c-29fe-47fb-a079-5fcbec211d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"funny\", \"not funny\"] = Field(\n",
    "        description=\"Decide if the joke is funny or not.\",\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n",
    "    )\n",
    "\n",
    "# Create evaluator (LLM that returns Feedback)\n",
    "evaluator = llm.with_structured_output(Feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d20c80-2c7f-47eb-9224-e636c84deebc",
   "metadata": {},
   "source": [
    "### Define Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffdef9-cb94-48b6-8d25-5ceb36d54275",
   "metadata": {},
   "source": [
    "    GENERATOR: Creates content (with optional feedback incorporation).\n",
    "    \n",
    "    Two modes:\n",
    "    1. First attempt: Generate from scratch\n",
    "    2. Retry: Incorporate feedback from evaluator\n",
    "    \n",
    "    Visual Loop:\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Iteration 1:                                                 â”‚\n",
    "    â”‚   No feedback â†’ Generate: \"Why did cat cross road?\"          â”‚\n",
    "    â”‚                                                              â”‚\n",
    "    â”‚ Evaluator: \"Not funny - too generic\"                         â”‚\n",
    "    â”‚                                                              â”‚\n",
    "    â”‚ Iteration 2:                                                 â”‚\n",
    "    â”‚   With feedback â†’ Generate: \"Why don't cats play poker?      â”‚\n",
    "    â”‚                             Too many cheetahs!\"              â”‚\n",
    "    â”‚                                                              â”‚\n",
    "    â”‚ Evaluator: \"Funny!\" â†’ Accept                                 â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    Args:\n",
    "        state: Contains topic and optional feedback\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with new joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8edb2b0e-2175-4fd3-95dd-90183825838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_generator(state: State):\n",
    "    if state.get(\"feedback\"):\n",
    "        # RETRY MODE: Incorporate feedback\n",
    "        msg = llm.invoke(\n",
    "            f\"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}\"\n",
    "        )\n",
    "    else:\n",
    "        # FIRST ATTEMPT: Generate from scratch\n",
    "        msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    \n",
    "    return {\"joke\": msg.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9a2bd-d462-4480-a908-de60e6af5ed6",
   "metadata": {},
   "source": [
    "    EVALUATOR: Grades the content quality.\n",
    "    \n",
    "    This node decides if we're done or need to retry.\n",
    "    \n",
    "    Process:\n",
    "    1. Take the current joke\n",
    "    2. Ask LLM to evaluate it\n",
    "    3. Get structured response (grade + feedback)\n",
    "    4. Return evaluation results\n",
    "    \n",
    "    Returns:\n",
    "        - funny_or_not: \"funny\" or \"not funny\"\n",
    "        - feedback: How to improve (if not funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb3954df-5970-4413-936a-eb8b614ca526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_evaluator(state: State):\n",
    "    grade = evaluator.invoke(f\"Grade the joke {state['joke']}\")\n",
    "    return {\n",
    "        \"funny_or_not\": grade.grade,\n",
    "        \"feedback\": grade.feedback\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3eebcd-91cb-4518-ada8-b1f6c93bf108",
   "metadata": {},
   "source": [
    "### Define Routing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f98d4-1b8c-43aa-bf3e-21fe2ac1e25a",
   "metadata": {},
   "source": [
    "    Route based on evaluation: Accept or Reject with feedback.\n",
    "    \n",
    "    Decision Tree:\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚Evaluator  â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚                      â”‚\n",
    "              â–¼                      â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Funny   â”‚          â”‚ Not Funny   â”‚\n",
    "        â”‚  â†’ END   â”‚          â”‚ â†’ Retry     â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â””â”€â”€â–º Loop back to Generator\n",
    "    \n",
    "    Returns:\n",
    "        \"Accepted\": Go to END (we're done!)\n",
    "        \"Rejected + Feedback\": Go back to generator (try again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86d1c4a8-4438-448b-a912-d61534d74b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_joke(state: State):\n",
    "    if state[\"funny_or_not\"] == \"funny\":\n",
    "        return \"Accepted\"\n",
    "    elif state[\"funny_or_not\"] == \"not funny\":\n",
    "        return \"Rejected + Feedback\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61fa310-c92f-4f2a-9db5-3149e035c0fc",
   "metadata": {},
   "source": [
    "### Build Evaluator-Optimizer Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80be8d90-7c63-40f1-9969-d12baf143bf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Running Evaluator-Optimizer Pattern]\n",
      "Final joke: <think>\n",
      "Okay, the user wants a joke about cats. Let me think about common cat-related humor. Cats are known for their independence, knocking things over, and their love for boxes. Maybe play on those traits.\n",
      "\n",
      "Hmm, what's a typical situation where a cat causes a funny problem? Maybe something with their curiosity leading to a mishap. Like knocking over something important. Maybe a keyboard? Because that's relatable.\n",
      "\n",
      "Wait, the joke should have a setup and a punchline. Let me try: \"Why did the cat get stuck in the microwave?\" Maybe the punchline is something about being a \"purr-fect fit.\" Wait, that's a play on \"perfect fit\" and \"pur.\" But maybe that's too forced. Let me think again.\n",
      "\n",
      "Alternatively, maybe a pun on \"meow.\" Like, \"Why don't cats ever get cold? Because they always have nine lives and a sweater!\" No, that's not very funny. Maybe something about their behavior. \n",
      "\n",
      "How about this: \"Why did the cat bring a ladder to the party? Because it heard the drinks were on the house!\" Wait, that's more about a ladder. Maybe not. \n",
      "\n",
      "Wait, the user might want something simple and clean. Let me go back to the first idea. \"Why did the cat get stuck in the microwave? Because it wanted to be a purr-fect fit!\" Maybe that's okay. Or maybe \"Why did the cat cross the road? To get to the other... meow!\" That's a play on the classic \"Why did the chicken cross the road?\" joke. \n",
      "\n",
      "Alternatively, \"Why don't cats ever get cold? They have nine lives and a sweater!\" Maybe that's better. Or maybe a joke about their obsession with boxes. \"Why did the cat bring a box to the party? Because it heard the drinks were on the house!\" Wait, that's similar to the ladder one. \n",
      "\n",
      "Wait, maybe the first idea is better. Let me check if the pun works. \"Purr-fect fit\" plays on \"perfect fit\" and \"pur.\" It's a bit forced, but maybe acceptable. Alternatively, \"Why did the cat bring a ladder to the microwave? To reach the top shelf!\" No, that's not funny. \n",
      "\n",
      "Alternatively, \"Why did the cat knock over the vase? Because it wanted to make a statement... and also because it's a bit clumsy!\" Maybe that's too long. \n",
      "\n",
      "I think the first idea with the microwave and purr-fect fit is okay. Let me go with that. Or maybe another angle. \"Why did the cat refuse to play hide and seek? Because it's always found!\" That's a bit better. \n",
      "\n",
      "Wait, maybe the user wants something more classic. Let me think of a common cat joke structure. Maybe something like, \"Why did the cat go to the doctor? Because it had a broken claw. The doctor said, 'You need to take it easy, you're a little fluffball!'\" Not very funny. \n",
      "\n",
      "Alternatively, \"Why did the cat sit on the computer? To keep the mouse in check!\" That's a play on \"mouse\" as the computer mouse and the animal. That's a good one. \n",
      "\n",
      "Yes, that's a classic pun. Let me check if that's original enough. Maybe. Alternatively, \"Why did the cat bring a ladder to the party? Because it heard the drinks were on the house!\" Wait, that's similar to the previous one. \n",
      "\n",
      "I think the \"mouse\" joke is better. Let me go with that. So the joke would be: \"Why did the cat sit on the computer? To keep the mouse in check!\" That's a good pun. Alternatively, \"Why did the cat bring a ladder to the party? Because it heard the drinks were on the house!\" Maybe that's better. \n",
      "\n",
      "Wait, the \"mouse\" joke is more clever. Let me confirm. The computer mouse is a common term, so the pun works. Yeah, that's a solid joke. I'll go with that.\n",
      "</think>\n",
      "\n",
      "Why did the cat bring a ladder to the party?  \n",
      "Because it heard the drinks were on the *house*! ğŸ¾ğŸ·  \n",
      "\n",
      "*(Bonus: The cat also knocked over the punch bowl.)* ğŸ˜¸\n",
      "Quality: funny\n"
     ]
    }
   ],
   "source": [
    "optimizer_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "optimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\n",
    "optimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n",
    "\n",
    "# Add edges\n",
    "optimizer_builder.add_edge(START, \"llm_call_generator\")\n",
    "optimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\n",
    "\n",
    "# Conditional edge creates the LOOP\n",
    "optimizer_builder.add_conditional_edges(\n",
    "    \"llm_call_evaluator\",\n",
    "    route_joke,\n",
    "    {\n",
    "        \"Accepted\": END,                             # Good enough â†’ stop\n",
    "        \"Rejected + Feedback\": \"llm_call_generator\"  # Not good â†’ retry\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "optimizer_workflow = optimizer_builder.compile()\n",
    "\n",
    "print(\"\\n[Running Evaluator-Optimizer Pattern]\")\n",
    "state = optimizer_workflow.invoke({\"topic\": \"Cats\"})\n",
    "print(f\"Final joke: {state['joke']}\")\n",
    "print(f\"Quality: {state['funny_or_not']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b7665-1555-4bb0-86dd-015a95b13094",
   "metadata": {},
   "source": [
    "## PATTERN 6: AGENTS (Tool-Using LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c84f8-9479-4a51-842d-33f37c45d1af",
   "metadata": {},
   "source": [
    "    AGENTS: LLMs that decide when and how to use tools.\n",
    "    \n",
    "    Use Case: LLMs need to perform actions (calculations, API calls, etc.)\n",
    "    \n",
    "    Flow:\n",
    "             START\n",
    "               â”‚\n",
    "               â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚   LLM   â”‚ â† \"I need to calculate 3+4\"\n",
    "          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
    "          â”‚         â”‚\n",
    "          â–¼         â–¼\n",
    "      Use Tool   Respond\n",
    "          â”‚       Directly\n",
    "          â”‚         â”‚\n",
    "          â–¼         â”‚\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "      â”‚Executeâ”‚     â”‚\n",
    "      â”‚Tool   â”‚     â”‚\n",
    "      â””â”€â”€â”€â”¬â”€â”€â”€â”˜     â”‚\n",
    "          â”‚         â”‚\n",
    "          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚   LLM   â”‚ â† \"The answer is 7\"\n",
    "          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "              END\n",
    "    \n",
    "    This is the AGENTIC LOOP: Think â†’ Act â†’ Observe â†’ Think â†’ ...\n",
    "    \n",
    "    WHY AGENTS MATTER:\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Problem: LLMs are bad at math\n",
    "      User: \"What is 3 + 4?\"\n",
    "      LLM: \"3 + 4 = 7.2\"  â† Sometimes wrong!\n",
    "    \n",
    "    Solution: Give LLM tools\n",
    "      User: \"What is 3 + 4?\"\n",
    "      LLM: \"I'll use the add tool\"\n",
    "      Tool: add(3, 4) = 7  â† Always correct!\n",
    "      LLM: \"The answer is 7\"\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \n",
    "    Real-World Examples:\n",
    "    - Calculator agent: Math questions â†’ Use calculator tools\n",
    "    - Research agent: Questions â†’ Search web â†’ Synthesize\n",
    "    - Database agent: Queries â†’ SQL tools â†’ Format results\n",
    "    - API agent: Tasks â†’ Call APIs â†’ Process responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3cae9d2-8b58-4c18-9c4c-f742e0e45884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain.messages import SystemMessage, HumanMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229c6b5-33e0-4fa5-b6e1-f06f7848df67",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc508c-abb7-46ae-b662-0b7bdcfc90d3",
   "metadata": {},
   "source": [
    "    Tools are functions the LLM can call.\n",
    "    \n",
    "    The @tool decorator:\n",
    "    1. Extracts function signature\n",
    "    2. Reads docstring (LLM uses this to understand the tool!)\n",
    "    3. Creates a schema the LLM can understand\n",
    "    \n",
    "    CRITICAL: Good docstrings = LLM knows when to use the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81cd1405-70dc-4238-a928-fa6877f53fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply `a` and `b`.\n",
    "    \n",
    "    Use this when the user asks to multiply numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First integer to multiply\n",
    "        b: Second integer to multiply\n",
    "        \n",
    "    Returns:\n",
    "        The product of a and b\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds `a` and `b`.\n",
    "    \n",
    "    Use this when the user asks to add or sum numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First integer to add\n",
    "        b: Second integer to add\n",
    "        \n",
    "    Returns:\n",
    "        The sum of a and b\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide `a` by `b`.\n",
    "    \n",
    "    Use this when the user asks to divide numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: Numerator (number to be divided)\n",
    "        b: Denominator (number to divide by)\n",
    "        \n",
    "    Returns:\n",
    "        The quotient of a divided by b\n",
    "    \"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b24ce-e0d7-4402-8d6c-3cbe74e98f11",
   "metadata": {},
   "source": [
    "### Bind Tools to LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa72d8-1475-43cd-b348-c517689ae99d",
   "metadata": {},
   "source": [
    "    After binding, the LLM knows about these tools and can:\n",
    "    1. Decide when to use them\n",
    "    2. Choose the right tool for the task\n",
    "    3. Provide arguments in the correct format\n",
    "    \n",
    "    Visual:\n",
    "        Regular LLM:\n",
    "            Input: \"What is 3 + 4?\"\n",
    "            Output: \"3 + 4 equals 7\" (just text)\n",
    "        \n",
    "        LLM with tools:\n",
    "            Input: \"What is 3 + 4?\"\n",
    "            Output: AIMessage(\n",
    "                tool_calls=[{\n",
    "                    \"name\": \"add\",\n",
    "                    \"args\": {\"a\": 3, \"b\": 4},\n",
    "                    \"id\": \"call_123\"\n",
    "                }]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5560a1fb-d7b4-4e63-bb57-403d53773077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tools\n",
    "tools = [add, multiply, divide]\n",
    "\n",
    "# Create lookup dictionary (for executing tools later)\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "# Result: {\"add\": <add_tool>, \"multiply\": <multiply_tool>, \"divide\": <divide_tool>}\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a22e00-cf0f-49a3-9252-546ed48ca3dd",
   "metadata": {},
   "source": [
    "### Define Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade75bf-60f6-44b9-b262-7518809ff6d8",
   "metadata": {},
   "source": [
    "    LLM NODE: The \"brain\" that decides what to do.\n",
    "    \n",
    "    This node:\n",
    "    1. Looks at conversation history\n",
    "    2. Decides whether to:\n",
    "       - Call a tool (e.g., \"I need to use add(3, 4)\")\n",
    "       - Respond directly (e.g., \"The answer is 7\")\n",
    "    \n",
    "    Returns AIMessage with either:\n",
    "    - tool_calls: List of tools to call (if needs tools)\n",
    "    - content: Direct text response (if has answer)\n",
    "    \n",
    "    Example Flow:\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Messages: [HumanMessage(\"What is 3 + 4?\")]\n",
    "    â†“\n",
    "    LLM thinks: \"I should use the add tool for this\"\n",
    "    â†“\n",
    "    Returns: AIMessage(tool_calls=[{name:\"add\", args:{a:3,b:4}}])\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e316e981-9187-49a6-92fc-7eb285477f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(state: MessagesState):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]  # Add conversation history\n",
    "            )\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e56d1-b025-4e29-aec1-86d783b1e93e",
   "metadata": {},
   "source": [
    "    TOOL NODE: The \"hands\" that execute actions.\n",
    "    \n",
    "    This node:\n",
    "    1. Extracts tool calls from last AI message\n",
    "    2. For each tool call:\n",
    "       - Look up tool by name\n",
    "       - Execute with provided arguments\n",
    "       - Create ToolMessage with result\n",
    "    3. Return ToolMessages to append to conversation\n",
    "    \n",
    "    Detailed Process:\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Input: Last message has tool_calls=[{name:\"add\", args:{a:3,b:4}, id:\"123\"}]\n",
    "    \n",
    "    Step 1: Extract tool call\n",
    "        tool_call = {name: \"add\", args: {a:3, b:4}, id: \"123\"}\n",
    "    \n",
    "    Step 2: Look up tool\n",
    "        tool = tools_by_name[\"add\"]  â†’ <add function>\n",
    "    \n",
    "    Step 3: Execute tool\n",
    "        result = add.invoke({a: 3, b: 4})  â†’ 7\n",
    "    \n",
    "    Step 4: Create result message\n",
    "        ToolMessage(content=\"7\", tool_call_id=\"123\")\n",
    "    \n",
    "    Output: {messages: [ToolMessage(\"7\")]}\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \n",
    "    The tool_call_id links the result back to the original request!\n",
    "    This is how the LLM knows which tool call this result belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8bd531d-15c5-4bcd-84d3-ff4165fa610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_node(state: dict):\n",
    "    result = []\n",
    "    \n",
    "    # Process each tool call\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        # Look up tool by name\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        \n",
    "        # Execute the tool\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        \n",
    "        # Create ToolMessage with result\n",
    "        result.append(\n",
    "            ToolMessage(\n",
    "                content=str(observation),      # The actual result\n",
    "                tool_call_id=tool_call[\"id\"]   # Links to original request\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f403ccd-5967-4370-9834-7efdbb47be3b",
   "metadata": {},
   "source": [
    "### Define Routing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceed15b-5ec7-44e9-aecd-55b5f8a46c60",
   "metadata": {},
   "source": [
    "    Decide: Execute tool OR finish?\n",
    "    \n",
    "    This is the AGENTIC DECISION:\n",
    "    - If LLM wants to use tools â†’ Execute them\n",
    "    - If LLM has final answer â†’ We're done\n",
    "    \n",
    "    Decision Tree:\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                      â”‚Last message  â”‚\n",
    "                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚                         â”‚\n",
    "                â–¼                         â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚Has tool_calls?â”‚         â”‚No tool_calls?â”‚\n",
    "        â”‚     YES       â”‚         â”‚      NO      â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚                        â”‚\n",
    "                â–¼                        â–¼\n",
    "        Go to tool_node              Go to END\n",
    "        (execute tools)          (done - have answer)\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \n",
    "    Args:\n",
    "        state: Contains message history\n",
    "        \n",
    "    Returns:\n",
    "        \"tool_node\": If LLM wants to use tools\n",
    "        END: If LLM has final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d0bc4cc-be83-4eb0-a11f-fbc0346b75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if LLM made a tool call\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"  # Execute the tools\n",
    "    \n",
    "    return END  # LLM provided final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bd0ed-cf48-41df-a467-eda1cc9a86d7",
   "metadata": {},
   "source": [
    "### Build Agent Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb073f-3db3-42dd-83ec-0ad90ffb9869",
   "metadata": {},
   "source": [
    "    Agent Graph Structure:\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "             START\n",
    "               â”‚\n",
    "               â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚llm_call â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚\n",
    "               â”‚                â”‚\n",
    "        [should_continue?]      â”‚\n",
    "               â”‚                â”‚\n",
    "          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”           â”‚\n",
    "          â–¼         â–¼           â”‚\n",
    "      tool_node    END          â”‚\n",
    "          â”‚                     â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    The LOOP is critical:\n",
    "    - tool_node â†’ llm_call allows multiple tool calls\n",
    "    - LLM can use one tool, see result, then use another tool\n",
    "    - Continues until LLM has final answer\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52793890-9075-44b3-a6ad-e849ca6a493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Running Agent Pattern]\n",
      "\n",
      "Conversation trace:\n",
      "============================================================\n",
      "\n",
      "--- Message 1 ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "\n",
      "--- Message 2 ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user wants to add 3 and 4. Let me check the available functions. There's the 'add' function which takes two integers, a and b. Since the user is asking to add 3 and 4, I should call the add function with a=3 and b=4. That should return the sum, which is 7. I need to make sure I use the correct parameters and format the tool call properly.\n",
      "</think>\n",
      "Tool Calls:\n",
      "  add (54b5fb3a-652e-4fe8-9a11-71d16bde775c)\n",
      " Call ID: 54b5fb3a-652e-4fe8-9a11-71d16bde775c\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "\n",
      "--- Message 3 ---\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "7\n",
      "\n",
      "--- Message 4 ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user asked to add 3 and 4. I called the add function with a=3 and b=4. The response from the tool was 7. So I just need to present that result clearly. Let me check if there's anything else needed, but since the user's request was straightforward, the answer should be the sum, which is 7. No further actions or tool calls are necessary here.\n",
      "</think>\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "\n",
    "# Conditional edge: tool_node or END?\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END]\n",
    ")\n",
    "\n",
    "# CRITICAL LOOP: After tool execution, go back to LLM\n",
    "# This allows the agent to:\n",
    "# 1. See the tool result\n",
    "# 2. Decide if more tools are needed\n",
    "# 3. Format the final answer\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "# Compile\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "print(\"\\n[Running Agent Pattern]\")\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "result = agent.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"\\nConversation trace:\")\n",
    "print(\"=\" * 60)\n",
    "for i, m in enumerate(result[\"messages\"], 1):\n",
    "    print(f\"\\n--- Message {i} ---\")\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55125fa-c00e-4918-8a5e-adfd421c78ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Ollama)",
   "language": "python",
   "name": "ollama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
